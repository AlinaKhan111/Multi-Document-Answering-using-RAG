# Multi-Document Question Answering System using RAG

A Retrieval-Augmented Generation (RAG) based application that allows users to upload multiple documents (PDF, DOCX, CSV) and ask natural language questions.
The system retrieves the most relevant content using vector search (FAISS) and generates grounded answers using LLMs.


## ğŸŒ Live Demo

 **Try the App Live:**  
ğŸ‘‰ [Hugging Face Spaces â€“ Multi-Document RAG](https://huggingface.co/spaces/AlinaaaKhannn/rag_multi_document)



## ğŸš€ Features

- Upload multiple documents at once

- Supports PDF, DOCX, CSV

- Semantic search using Sentence Transformers + FAISS

- Retrieval-Augmented Generation (RAG) pipeline

- LLM routing between Groq (LLaMA 3) and Google Gemini

- Retrieval-only demo mode (LLM optional)

- Source-aware answers with document & chunk references

- Deployed on Hugging Face Spaces


## ğŸ§  RAG Architecture


```mermaid
flowchart LR
    User[User] -->|Question| UI[Streamlit UI]

    UI --> Ingest[Document Ingestion]
    Ingest --> Chunk[Text Chunking]

    Chunk --> Embed[Embedding Model]
    Embed --> FAISS[FAISS Vector Index]

    User -->|Query| QueryEmbed[Query Embedding]
    QueryEmbed --> FAISS

    FAISS --> Retrieve[Top-K Chunks]
    Retrieve --> Prompt[RAG Prompt Builder]

    Prompt --> LLM[LLM- Groq or Gemini]
    LLM --> Answer[Final Answer]

    Answer --> UI
```

## âš™ï¸ Tech Stack

- **Frontend:** Streamlit

- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2`

- **Vector DB:** FAISS

- **LLMs:**

     - **Groq** â€“ `llama-3.1-8b-instant`

     - **Google Gemini** â€“ `gemini-flash-latest`

- **Language:** Python


## âš™ï¸ Configuration

All configurations are centralized in `config.py.`

## Environment Variables

Create a `.env` file:

```bash
GEMINI_API_KEY=your_gemini_api_key
GROQ_API_KEY=your_groq_api_key
```

## ğŸ›  Installation & Setup (Local)

1ï¸. Clone the repository
```bash
git clone https://github.com/AlinaKhan111/Multi-Document-Answering-using-RAG.git
cd Multi-Document-Answering-using-RAG
```

2ï¸. Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   
```

3ï¸. Install dependencies
```bash
pip install -r requirements.txt
```

4ï¸. Run the app
```bash
streamlit run app.py
```

##  How It Works

1. User uploads one or more documents.

2. Documents are converted into plain text.

3. Text is split into overlapping chunks.

4. Chunks are embedded and indexed using FAISS.

5. User query is embedded and matched against chunks.

6. Top-K relevant chunks are passed to the LLM.

7. LLM generates an answer only from retrieved context.


## ğŸ› LLM Modes

- **Groq (LLaMA-3)** â†’ Fast, cost-efficient

- **Gemini** â†’ Strong reasoning

- **Retrieval-only mode** â†’ Useful for demos and debugging


## ğŸ“š Example Use Cases

- Academic notes Q&A

- Resume or policy document analysis

- Research paper exploration

- Multi-file knowledge assistant


## ğŸŒ Deployment (Hugging Face Spaces)

This project is deployed using Streamlit on **Hugging Face Spaces.**

**Notes:**

- Secrets are added via HF Spaces â†’ Settings â†’ Secrets

- Uses lightweight models for memory efficiency

- No code changes required for deployment


## ğŸš§ Limitations

- In-memory FAISS (no persistence)

- No OCR for scanned PDFs

- Single-user session (Streamlit default)


## ğŸ”® Future Improvements

- Persistent vector store (Chroma / Pinecone)

- Streaming responses

- OCR support for scanned PDFs

- Chat history & memory

- Authentication & access control


## ğŸ§‘â€ğŸ’» Skills Demonstrated

- Retrieval-Augmented Generation (RAG)

- Vector databases & semantic search

- LLM prompt engineering

- Multi-LLM orchestration

- Modular Python architecture

- Streamlit deployment

- Hugging Face Spaces